# ######################################################################################################################
[DEFAULT] # Test and Training defaults and General Options
# ----------------------------------------------------------------------------------------------------------------------
# The parameters [output_dir, name] respectively [train_dir] as well as
# [continue_training, test_dir, checkpoint] are command line arguments
# They can be specified via CL when running the script or will default to sensible values.

# ---------> Training & Testing: Data and Network architecture
shuffle = True
augment = True
resample_n = None
keep_prob = 0.9
batch_size = 1

# Path to dataset that will be used by default (overwrite in each section)
dataset = /home/hornebeh/proj_tf_unet/data/hdf5/trainset.h5
# data_layer type
data_layer_type = hdf5
shape_img = [512, 512, 4]
shape_label = [512, 512, 1]
shape_weights = [512, 512, 1]
n_class = 2
#shape_img = [1024, 1024, 4]
#shape_label = [1024, 1024, 1]
#shape_weights = [1024, 1024, 1]

# None or [sizeX, sizeY]  # [512, 512]
resize = None
# None / scale / center_crop or random_crop (default)
resize_method = None
Aleatoric loss: Cleanup, config, sigma computation.
# working configurations for 12 GB GPU RAM:
# size 512x512: batch_size= 1 | 2 | 8 ; n_start_features= 64 | 64 | 16
n_start_features = 64
n_contracting_blocks = 5

# for aleatoric uncertainty
# unet 1x512x512|64|5 with val and mean_square regularizrt:
# Al:300 = ~0.5ips / 50 = ~1.0ips / 10 = ~1.1ips
aleatoric_sample_n = 50
aleatoric_distr = None
# adds a regularizer term to the aleatoric loss
aleatoric_reg = None

# None or batch_norm (tc.layers.batch_norm)
norm_fn = tc.layers.batch_norm

# None (for no normalization or default params) or True to load params from below
norm_fn_params = True
# True / False or None if it should correspond to current phase
# True corresponds to immediate normalization
norm_fn_param_is_training = True
# for stability
norm_fn_param_zero_debias_moving_mean = True
# default is 0.999, lower is supposed to make normalization params more stable
norm_fn_param_decay = 0.9

prefetch_threads = 12
prefetch_n = 32


# ######################################################################################################################
[TRAIN]
# ----------------------------------------------------------------------------------------------------------------------
dataset = /home/hornebeh/proj_tf_unet/data/hdf5/trainset.h5
train_name = None

optimizer = Adam
learning_rate_init = 0.0001
learning_rate_decay = 0.5
max_iter = 30000
saver_interval = 1000
continue = True

# ---------> Training OVERRIDE defaults
# keep_prob = 1.0
#shuffle = True
#augment = True
#resample_n = 1000
# batch_size = 1


# ######################################################################################################################
[TEST]
# ----------------------------------------------------------------------------------------------------------------------
dataset = /misc/lmbraid19/hornebeh/std/projects/remote_deployment/win_tf_unet/data/hdf5/testset.h5
n_samples = 19

# ---------> Testing OVERRIDE defaults
# keep_prob = 1.0
shuffle = False
augment = False
resample_n = None
# batch_size = 1

# None or [sizeX, sizeY]
;resize = [512, 512]
# None / "scale" / "center_crop" or "random_crop" (default)
;resize_method = center_crop


# ######################################################################################################################
[VAL]
# ----------------------------------------------------------------------------------------------------------------------
dataset = /home/hornebeh/proj_tf_unet/data/hdf5/valset.h5
val_intervall = 20

val_intervall_sample = None
n_samples = None
val_dir = None

# ---------> Validation OVERRIDE defaults
shuffle = False
augment = False
resample_n = None
# batch_size = 1
# keep_prob = 1.0

# None or [sizeX, sizeY]
;resize = [512, 512]
# None / "scale" / "center_crop" or "random_crop" (default)
;resize_method = center_crop