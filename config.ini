# ######################################################################################################################
[DEFAULT] # Test and Training defaults and General Options
# ----------------------------------------------------------------------------------------------------------------------
# The parameters [output_dir, name] respectively [train_dir] as well as
# [continue_training, test_dir, checkpoint] are command line arguments
# They can be specified via CL when running the script or will default to sensible values.

# ---------> Training & Testing: Data and Network architecture
shuffle = True
augment = True
resample_n = None
keep_prob = 0.8
batch_size = 1

# Path to dataset that will be used by default (overwrite in each section)
dataset = /home/hornebeh/proj_tf_unet/data/hdf5/trainset.h5
# data_layer type
data_layer_type = hdf5
shape_img = [512, 512, 4]
shape_label = [512, 512, 1]
shape_weights = [512, 512, 1]
n_class = 2

# None or [sizeX, sizeY]  # [512, 512]
resize = None
# None / scale / center_crop or random_crop (default)
resize_method = None

# working configurations for 12 GB GPU RAM:
# size 512x512: batch_size= 1 | 2 | 8 ; n_start_features= 64 | 64 | 16
n_start_features = 64
n_contracting_blocks = 5

# load checkpoint from specific iteration (for test or continued training)
global_step = None

# for aleatoric uncertainty
# unet 1x512x512|64|5 with val and mean_square regularizrt:
# Al:300 = ~0.5ips / 50 = ~1.0ips / 10 = ~1.1ips
aleatoric_sample_n = None
aleatoric_distr = None
# adds a regularizer term to the aleatoric loss
aleatoric_reg = None

# None or batch_norm (tc.layers.batch_norm)
norm_fn = tc.layers.batch_norm

# None (for no normalization or default params) or True to load params from below
norm_fn_params = True
# True / False or None if it should correspond to current phase
# True corresponds to immediate normalization
norm_fn_param_is_training = True
# for stability
norm_fn_param_zero_debias_moving_mean = True
# default is 0.999, lower is supposed to make normalization params more stable
norm_fn_param_decay = 0.9

prefetch_threads = 12
prefetch_n = 32


# ######################################################################################################################
[TRAIN]
# ----------------------------------------------------------------------------------------------------------------------
dataset = /home/hornebeh/proj_tf_unet/data/hdf5/trainset.h5
# allows to add a training name to the train folder
train_name = None

optimizer = Adam
# a good value is 0.0001 (1e-4)
learning_rate_init = 0.0001
learning_rate_decay = 0.5
max_iter = 60000
saver_interval = 1000
continue = True

# ---------> Training OVERRIDE defaults
keep_prob = 1.0
shuffle = True
augment = True
# aleatoric_sample_n = 200


# ######################################################################################################################
[TEST]
# ----------------------------------------------------------------------------------------------------------------------
dataset = /misc/lmbraid19/hornebeh/std/projects/remote_deployment/win_tf_unet/data/hdf5/testset.h5
# the number of samples that should be tested
n_samples = 19

# ---------> Testing OVERRIDE defaults
# specify a certain iteration to test (checkpoint needs to be available!)
#global_step = None
keep_prob = 1.0
aleatoric_sample_n = 50
shuffle = False
augment = False
# during testtime, resample_n represents epistemic uncertainty samples
resample_n = 20


# ######################################################################################################################
[VAL]
# ----------------------------------------------------------------------------------------------------------------------
dataset = /home/hornebeh/proj_tf_unet/data/hdf5/valset.h5
val_intervall = 20

val_intervall_sample = None
n_samples = None
val_dir = None

# ---------> Validation OVERRIDE defaults
shuffle = False
augment = False
resample_n = None