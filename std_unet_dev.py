# UNET TRAINING AND TESTING
# =========================
#
# Python (commandline) script for training and/or testing a Unet.
#
# Author: Hannes Horneber
# Date: 2018-03-18

from __future__ import division, print_function

import os
import sys
import argparse
import configparser
import shutil
import time
import datetime
import matplotlib;
matplotlib.use('Agg')  # set this before any other module makes use of matplotlib (and sets it)

import numpy as np
from timeit import default_timer as timer
from collections import OrderedDict

# check how long tensorflow import takes
print('Importing tensorflow ...')
t_start = timer()
import tensorflow as tf
print('Elapsed time: %.4f s' % (timer() - t_start))
from tensorflow.python import debug as tf_debug
from tensorflow import contrib as tc

import logging
from util import config_util
from util import img_util
from util import calc
from util import logs
from util import filesys
from util import tf_helpers
# from unet import model
from unet import model_dev as model
from unet import data_layers
from util.tfutils import SimpleTrainer

# ######################################################################################################################
# LOGGING OPTIONS
# ---------------
# output log to output folder [script-location]/log
logs.initDebugLogger(os.path.join(sys.path[0], "log"), script_name="std_unet")
# output error log to output folder [script-location]/log
logs.initErrorLogger(os.path.join(sys.path[0], "log"), script_name="std_unet-err")
# console logger - depending on your settings this might not be necessary and produce doubled output
logs.initConsoleLogger()

# ######################################################################################################################
# COMMANDLINE ARGUMENTS PARSER
# ----------------------------
# all required flags are set to False to be able to run this script from environments w/o commandline options
# change defaults or override commandline args below to change what you are providing

args = argparse.ArgumentParser(description='UNet Training and Testing')
# TRAINING & TESTING arguments
args.add_argument('--dataset', '-d', metavar='dataset', required=False,
                    help='Path to the dataset that will be used for training/testing',
                    default="/home/hornebeh/proj_tf_unet/data/tfrecord/1024x1024_rgbi/train.tfrecords")
args.add_argument('--checkpoint', '-ckp', metavar='checkpoint', required=False,
                    help='Provide a model checkpoint file, otherwise searching for last one in train_dir/checkpoints.' +
                         'For training: Trains from scratch if no checkpoint is found.' +
                         'For testing: Breaks if no checkpoint is found (model cannot initialize).',
                    default=None)
args.add_argument('--mode', '-m', metavar='mode', required=False,
                    help="CL parameter to activate phases ['train' or 'test'] " +
                         "or modes ['tfdbg': TensorFlow CLI debugger, 'debug': Additonal debug code]. " +
                         "To activate multiple phases, just concatenate strings in arbitrary order, " +
                         "e.g. ('traintest' or 'testtrain' or 'debugtrain').",
                    default=None)
args.add_argument('--code_copy_dir', '-cc', metavar='code_copy_dir', required=False,
                    help='If specified, script and model will be copied to this dir. Default: train_dir.',
                    default='train_dir')

#   for output of training / input of testing:
#       either output_dir/name ...
args.add_argument('--name', '-n', metavar='train_name', required=False,
                    help='The training session name (if not specified using time only)',
                    default=time.strftime("%Y-%m-%d_%H%M"))
args.add_argument('--output_dir', '-o', metavar='output_dir', required=False,
                    help='Model and training files are written to train_dir=\"output_dir/name/\".',
                    default='/home/hornebeh/proj_tf_unet/output/')
#       ... or a full train_dir path:
args.add_argument('--train_dir', '-t', metavar='train_dir', required=False,
                    help='Directory where models (checkpoints) are stored during training and loaded for testing. ' +
                         'Usually generated as train_dir=\"output_dir/name/\".' +
                         'Pass this if you don\'t want to specify name and dir separately ' +
                         '(e.g. testing an already trained network) - or if for some reason you want to use a ' +
                         'different directory than the one that is generated by default for training ' +
                         '(this overrides output_dir and name).',
                    default=None)

# TESTING ARGUMENTS
args.add_argument('--continue_training', '-c', metavar='continue_training', required=False,
                    help='If not None, will continue training from latest checkpoint. ' +
                         'Otherwise train_dir is recreated.',
                    default=False)

# TESTING ARGUMENTS
args.add_argument('--test_dir', '-p', metavar='test_dir', required=False,
                    help='Prediction is written to \"test_dir/\". ' +
                         'If not specified using prediction subfolder in train_dir.',
                    default=None)

args = args.parse_args()


# ######################################################################################################################
# SCRIPT ARGS / OVERRIDE ARGS
# ---------------------------
def _________________________OPTIONS___________________________(): pass  # dummy function for PyCharm IDE
# Most options are defined and loaded via a config.ini

# -> Override CLI arguments (for convenience when running from IDE)
PROJECT_DIR = '/misc/lmbraid19/hornebeh/std/projects/remote_deployment/win_tf_unet/'


# args.output_dir = '/home/hornebeh/proj_tf_unet/output/'
args.name = 'unet_' + time.strftime("%Y-%m-%d_%H%M") + '_sh-aug-kp09-60k'
# for new trainings:
args.name = 'unet_' + time.strftime("%d_%H%M") + '_' + config_util.keystr_from_config(section='TRAIN')

# args.name = 'overwrite'
# args.train_dir = PROJECT_DIR + 'output/' + 'unet_2018-04-05_1409_debug_batch_norm'
# args.train_dir = PROJECT_DIR + 'output/' + 'unet_2018-04-05_1425_debug_no_batch_norm'
#args.train_dir = PROJECT_DIR + 'output/' + 'unet_2018-04-05_1713_sh-aug-kp1-60k'
#args.train_dir = PROJECT_DIR + 'output/' + 'unet_2018-04-05_1717_sh-aug-kp09-60k'

# args.train_dir = PROJECT_DIR + 'output_scr/' + 'unet_2018-03-25_2021_augment_saver'
# args.train_dir = PROJECT_DIR + 'output_scr/' + 'unet_2018-03-28_1813_debug_no_droput'

# args.checkpoint = PROJECT_DIR + 'output_scr/' + unet_2018-03-22_2021_augment/checkpoints/snapshot-34000"
# args.checkpoint = PROJECT_DIR + 'output/' + 'unet_2018-04-05_1425_debug_no_batch_norm/checkpoints/snapshot-4000'

# args.dataset = PROJECT_DIR + "data/hdf5/std_data_v0_2_pdf/train/merged/train_dset_chunked.h5"
args.dataset = PROJECT_DIR + "data/hdf5/testset.h5"

# ######################################################################################################################
# SETUP VARS AND DIRS
# -------------------

# if opts.debug: logging.debug(str(os.environ))    # log environment (to check whether CUDA paths are set correctly etc.)
logging.debug('os.uname: %s' % (str(os.uname())))  # log uname to check which node code is running on

args.train_dir, args.continue_training = filesys.find_or_create_train_dir(
    args.name, args.output_dir, args.train_dir, args.continue_training)

args.test_dir = filesys.find_or_create_test_dir(
    args.test_dir, args.train_dir)

args.code_copy_dir = filesys.find_or_create_code_copy_dir(
    __file__, args.code_copy_dir, args.train_dir)


# ######################################################################################################################


class config_extended(config_util.config_decorator):
    """
    Wrapper class for control flow options and default config options.
    It extends
    """

    def __init__(self, config, default='DEFAULT'):
        self.config = config
        self.config_prox = config['DEFAULT']

    debug = True if args.mode is None else ('debug' in args.mode)
    tfdbg = False if args.mode is None else ('tfdbg' in args.mode)
    # ---------> Training
    train = True if args.mode is None else ('train' in args.mode)  # script will train
    # ---------> Testing
    test = True if args.mode is None else ('test' in args.mode)  # script will do testing

    tf_config = tf.ConfigProto(log_device_placement=False)
    if 'dacky' in os.uname()[1]:
        logging.info('Dacky: Running with memory usage limits')
        # change tf_config for dacky to use only 1 GPU
        tf_config.gpu_options.per_process_gpu_memory_fraction = 0.6
        os.environ["CUDA_VISIBLE_DEVICES"] = "1"
    else:
        # change tf_config for lmb_cluster so that GPU is visible and utilized
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"


# initialize options from config file
config = configparser.ConfigParser()
config.read(filesys.find_or_create_config_path(args.train_dir))

opts = config_extended(config, default='DEFAULT')
opts_test = config_util.config_decorator(config['TEST'])
opts_train = config_util.config_decorator(config['TRAIN'])

# output opts to console
print(config_util.opts_to_str(opts))


opts_test.norm_fn

# ######################################################################################################################
# TRAINING
# --------
def __________________________TRAIN______________________________(): pass  # dummy function for PyCharm IDE


# core code for training
def train_core(sess, net):
    logging.info('#-----------------------------------------------#')
    logging.info('#               Starting Training               #')
    logging.info('#-----------------------------------------------#')

    # create a tfutils.SimpleTrainer that handles the mainloop and manages checkpoints
    trainer = SimpleTrainer(session=sess, train_dir=args.train_dir)
    # load model for continued training (if None provided, searches in train_dir, if not found doesn't load)
    chkpt_loaded = trainer.load_checkpoint(args.checkpoint)
    # init variables if no checkpoint was loaded
    if not chkpt_loaded: sess.run(tf.group(tf.global_variables_initializer()))
    logging.info("Loaded variables from checkpoint" if chkpt_loaded else "Randomly initialized variables")

    # set train_op and summaries
    logging.info('Create Training step (set loss, summary and global_step)')
    train_op, global_step, loss, merged_summary = \
        net.create_train_op(opts_train.init_learning_rate, opts_train.optimizer,
                            img_summary=True)

    # in case any tf vars are not initialized. Specifically needed for ADAM if ADAM variables aren't stored/loaded
    tf_helpers.initialize_uninitialized(sess)

    # setup saver list
    saver_list = tf.global_variables()  # tf.trainable_variables() might not contain all relevant variables
    saver_list.append(global_step)  # for easy output: import pprint; pprint.pprint(saver_list)

    trainer.mainloop(
        max_iter=opts_train.max_iter,
        saver_interval=opts_train.saver_interval,
        saver_var_list=saver_list,
        train_ops=([train_op]),
        display_str_ops=[('Loss', loss)],
        display_interval=1,
        runstats_interval=100,
        trace_interval=100,
        summary_int_ops=[(1, merged_summary)]
    )


# core code for training with debug store
def train_debug(sess, net):
    logging.info('#-----------------------------------------------#')
    logging.info('#               Starting Training (debug)       #')
    logging.info('#-----------------------------------------------#')

    # create a tfutils.SimpleTrainer that handles the mainloop and manages checkpoints
    trainer = SimpleTrainer(session=sess, train_dir=args.train_dir)

    logging.info('Initializing or loading variables')
    # TODO Debug saving method
    saver = tf.train.Saver()
    save_dir = args.train_dir + os.sep + "save"
    save_path = save_dir + os.sep + "model.ckpt"
    if os.path.exists(save_path):
        try:
            logging.info("Attempt restore safe debug model from: %s" % save_dir)
            saver.restore(sess, save_dir + os.sep + "model.ckpt")
            chkpt_loaded = True
            logging.info(" -- Restored --")
        except:
            logging.info("Attempt restore with SimpleTrainer load from: %s" % args.checkpoint)
            # load model for continued training (if None provided, searches in train_dir, if not found doesn't load)
            chkpt_loaded = trainer.load_checkpoint(args.checkpoint)
            # init variables if no checkpoint was loaded
    else:
        chkpt_loaded = False

    if not chkpt_loaded: sess.run(tf.group(tf.global_variables_initializer()))
    logging.info("Loaded variables from checkpoint" if chkpt_loaded else "Randomly initialized variables")

    # set train_op and summaries
    logging.info('Create Training step (set loss, summary and global_step)')
    train_op, global_step, loss, merged_summary = \
        net.create_train_op(opts_train.init_learning_rate, opts_train.optimizer,
                            img_summary=True)

    # in case any tf vars are not initialized. Specifically needed for ADAM if ADAM variables aren't stored/loaded
    tf_helpers.initialize_uninitialized(sess)

    # setup saver list
    saver_list = tf.global_variables()  # tf.trainable_variables() might not contain all relevant variables
    saver_list.append(global_step)  # for easy output: import pprint; pprint.pprint(saver_list)

    trainer.mainloop(
        max_iter=opts_train.max_iter,
        saver_interval=opts_train.saver_interval,
        saver_var_list=saver_list,
        train_ops=([train_op]),
        display_str_ops=[('Loss', loss)],
        display_interval=1,
        runstats_interval=100,
        trace_interval=100,
        summary_int_ops=[(1, merged_summary)]
    )

    # TODO Debug saving method
    if not os.path.exists(save_dir):
        os.mkdir(save_dir)
    saver.save(sess, save_path)
    logging.info("Safe debug model saved: %s" % save_path)


# training with my own main_loop
def train_own(sess, net):
    logging.info('#-----------------------------------------------#')
    logging.info('#               Starting Training (debug)       #')
    logging.info('#-----------------------------------------------#')

    TRAIN_LOGDIR = 'trainlogs'
    CHECKPOINTS_DIR = 'save'
    logdir = os.path.join(args.train_dir, TRAIN_LOGDIR)
    save_dir = os.path.join(args.train_dir, CHECKPOINTS_DIR)  # args.train_dir + os.sep + "save"
    save_path = os.path.join(save_dir, "model.ckpt")  # save_dir + os.sep + "model.ckpt"

    logging.info('Initializing or loading variables')
    # TODO Debug saving method
    restore_saver = tf.train.Saver()
    if os.path.exists(save_path):
        try:
            logging.info("Restoring model from: %s" % save_dir)
            # load model for continued training (if None provided, searches in train_dir, if not found doesn't load)
            restore_saver.restore(sess, save_dir + os.sep + "model.ckpt")
            chkpt_loaded = True
            logging.info(" -- Restored --")
        except:
            chkpt_loaded = False  # if restoring fails
    else:
        chkpt_loaded = False  # if no saved checkpoint is found

    if not chkpt_loaded: sess.run(tf.group(tf.global_variables_initializer()))
    logging.info("Loaded variables from checkpoint" if chkpt_loaded else "Randomly initialized variables")

    # set train_op and summaries
    logging.info('Create Training step (set loss, summary and global_step)')
    train_op, global_step, loss, merged_summary = \
        net.create_train_op(opts_train.init_learning_rate, opts_train.optimizer,
                            img_summary=True)

    # in case any tf vars are not initialized. Specifically needed for ADAM if ADAM variables aren't stored/loaded
    tf_helpers.initialize_uninitialized(sess)

    # setup saver list
    # saver_list = tf.global_variables() # tf.trainable_variables() might not contain all relevant variables
    # saver_list.append(global_step) # for easy output: import pprint; pprint.pprint(saver_list)
    # saver = tf.train.Saver(saver_list)
    saver = tf.train.Saver()

    # MAIN LOOP
    global_step_value = sess.run(global_step)
    summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())
    summary_writer.add_session_log(tf.SessionLog(status=tf.SessionLog.START), global_step=global_step_value)

    t_start = timer()
    logging.info('Starting training with global_step=%s' % (str(global_step_value)))

    while global_step_value < opts_train.max_iter:
        summary, loss_val, _ = sess.run([merged_summary, loss, train_op])

        # output to summary and console
        summary_writer.add_summary(summary, global_step=global_step_value)
        print("# {0} {1:>8} | ".format(datetime.datetime.fromtimestamp(int(time.time())), global_step_value), end="")
        print("{0}:{1:11.4g}  ".format("loss", loss_val).ljust(20), end="")
        print("\t {0:.4f} s  ".format((timer() - t_start)), end="")
        print("", flush=True)

        # saver interval
        if global_step_value and (global_step_value % opts_train.saver_interval == 0):
            if not os.path.exists(save_dir): os.mkdir(save_dir)
            chkpt_save_path = save_path + "_" + str(global_step_value)
            saver.save(sess, save_path)
            logging.info("Checkpoint saved: %s" % chkpt_save_path)

        # next step
        global_step_value = global_step_value + 1

    summary_writer.close()
    # trainer.mainloop(
    #     max_iter=opts_train.max_iter,
    #     saver_interval=2000,
    #     saver_var_list=saver_list,
    #     train_ops=([train_op]),
    #     display_str_ops=[('Loss', loss)],
    #     display_interval=1,
    #     runstats_interval=100,
    #     trace_interval=100,
    #     summary_int_ops=[(1, merged_summary)]
    # )

    # TODO Debug saving method
    if not os.path.exists(save_dir):
        os.mkdir(save_dir)
    saver.save(sess, save_path)
    logging.info("Model saved: %s" % save_path)


def TRAIN(): pass


if opts.train:
    logging.info('####################################################################')
    logging.info('#                            TRAINING                              #')
    logging.info('####################################################################')

    # create network graph with data layer
    net = model.UNet(dataset_pth=args.dataset,
                     shape_img=opts_train.shape_img, shape_label=opts_train.shape_label, shape_weights=opts_train.shape_weights,
                     batch_size=opts_train.batch_size, shuffle=opts_train.shuffle, augment=opts_train.augment,
                     resize=opts_train.resize, resize_method=opts_train.resize_method,
                     data_layer_type=opts_train.data_layer_type,
                     n_contracting_blocks=opts_train.n_contracting_blocks, n_start_features=opts_train.n_start_features,
                     norm_fn=opts.norm_fn, normalizer_params=opts.norm_fn_params,
                     resample_n=opts_train.resample_n,
                     is_training=True, keep_prob=opts_train.keep_prob,
                     aleatoric_samples=opts_train.aleatoric_samples,
                     prefetch_n=opts_train.prefetch_n, prefetch_threads=opts_train.prefetch_threads,
                     debug=opts.debug, copy_script_dir=args.code_copy_dir
                     )

    with tf_debug.LocalCLIDebugWrapperSession(tf.Session(config=opts.tf_config)) if opts.tfdbg \
            else tf.Session(config=opts.tf_config) as sess:
        if not opts.debug:
            train_core(sess, net)
        else:
            train_debug(sess, net)
            # train_own(sess, net)

    logging.info('#-X---------------------------------------------#')
    logging.info('#                Finish Training                #')
    logging.info('#-----------------------------------------------#')


# ######################################################################################################################
# TESTING
# -------
def ___________________________TEST______________________________(): pass  # dummy function for PyCharm IDE


if opts.train: tf.reset_default_graph()


# core code for testing
def test_core(sess, net_test):
    logging.info('#-----------------------------------------------#')
    logging.info('#               Starting Testing                #')
    logging.info('#-----------------------------------------------#')

    # load model for testing (if None provided, searches in train_dir, if not found doesn't load)
    trainer = SimpleTrainer(session=sess, train_dir=args.train_dir)
    chkpt_loaded = trainer.load_checkpoint(args.checkpoint)
    # init variables if no checkpoint was loaded
    if not chkpt_loaded: sess.run(tf.group(tf.global_variables_initializer()))
    logging.info("Loaded variables from checkpoint" if chkpt_loaded else "Randomly initialized (!) variables")

    # ###########################################################################
    # RUN UNET
    # ###########################################################################
    logging.debug("predicting, sampling %s times, batch_size %s" % (opts_test.n_samples, opts_test.batch_size))

    for b in range(opts_test.n_samples):
        batch_img, batch_label, batch_activations, batch_prediction = sess.run(
            [net_test.batch_img, net_test.batch_label, net_test.output_mask, net_test.prediction])

        # out_img = np.squeeze(img_util.to_rgb(batch_activations))
        # img_util.save_image(out_img, "%s/img_%s_pred.png" % (args.test_dir, b))

        logging.debug('batch_activations: %s %s' % (str(batch_activations.shape), str(batch_activations.dtype)))
        logging.debug('batch_prediction: %s %s' % (str(batch_prediction.shape), str(batch_prediction.dtype)))
        logging.debug('batch_img: %s %s' % (str(batch_img.shape), str(batch_img.dtype)))
        logging.debug('batch_label: %s %s' % (str(batch_label.shape), str(batch_label.dtype)))

        r_batch_img = np.reshape(batch_img, [-1, batch_img.shape[2], batch_img.shape[3]])
        r_batch_label = np.reshape(batch_label, [-1, batch_label.shape[2], batch_label.shape[3]])
        r_batch_activations = np.reshape(batch_activations,
                                         [-1, batch_activations.shape[2], batch_activations.shape[3]])
        r_batch_prediction = np.reshape(batch_prediction, [-1, batch_prediction.shape[2]])

        out_img = np.concatenate((np.squeeze(img_util.to_rgb(r_batch_img)),
                                  np.squeeze(img_util.to_rgb(r_batch_label)),
                                  np.squeeze(img_util.to_rgb(r_batch_activations[..., 0, np.newaxis], normalize=True)),
                                  np.squeeze(img_util.to_rgb(r_batch_activations[..., 1, np.newaxis], normalize=True)),
                                  np.squeeze(img_util.to_rgb(r_batch_prediction[..., np.newaxis]))
                                  ), axis=1)

        img_util.save_image(out_img, "%s/img_%s.png" % (args.predict_dir, b))


        # ###########################################################################
        # CLOSE NET
        # ###########################################################################


def test_debug(sess, net_test):
    logging.info('#-----------------------------------------------#')
    logging.info('#               Starting Testing (debug)        #')
    logging.info('#-----------------------------------------------#')

    # load model for testing (if None provided, searches in train_dir, if not found doesn't load)
    logging.info("Attempt restore with SimpleTrainer load from: %s" % args.checkpoint)
    # load model for continued training (if None provided, searches in train_dir, if not found doesn't load)
    trainer = SimpleTrainer(session=sess, train_dir=args.train_dir)
    chkpt_loaded = trainer.load_checkpoint(args.checkpoint)

    # init variables if no checkpoint was loaded
    if not chkpt_loaded: sess.run(tf.group(tf.global_variables_initializer()))
    logging.info("Loaded variables from checkpoint" if chkpt_loaded else "Randomly initialized (!) variables")

    # in case any variables are not yet initialized
    # initialize_uninitialized(sess)

    # ###########################################################################
    # RUN UNET
    # ###########################################################################
    logging.debug("predicting, sampling %s times, batch_size %s" % (opts_test.n_samples, opts_test.batch_size))

    for b in range(opts_test.n_samples):
        batch_img, batch_label, batch_activations, batch_prediction = sess.run(
            [net_test.batch_img, net_test.batch_label, net_test.output_mask, net_test.prediction])

        # out_img = np.squeeze(img_util.to_rgb(batch_activations))
        # img_util.save_image(out_img, "%s/img_%s_pred.png" % (args.test_dir, b))

        logging.debug('batch_activations: %s %s' % (str(batch_activations.shape), str(batch_activations.dtype)))
        logging.debug('batch_prediction: %s %s' % (str(batch_prediction.shape), str(batch_prediction.dtype)))
        logging.debug('batch_img: %s %s' % (str(batch_img.shape), str(batch_img.dtype)))
        logging.debug('batch_label: %s %s' % (str(batch_label.shape), str(batch_label.dtype)))

        # logging.debug('describe prediction_samples: ' + str(stats.describe(batch_activations)))
        # logging.debug('describe prediction_samples[0]: ' + str(stats.describe(prediction_samples[0])))
        # out_img = img_util.combine_img_prediction(batch_img, batch_label, batch_activations)

        r_batch_img = np.reshape(batch_img, [-1, batch_img.shape[2], batch_img.shape[3]])
        r_batch_label = np.reshape(batch_label, [-1, batch_label.shape[2], batch_label.shape[3]])
        r_batch_activations = np.reshape(batch_activations,
                                         [-1, batch_activations.shape[2], batch_activations.shape[3]])
        r_batch_prediction = np.reshape(batch_prediction, [-1, batch_prediction.shape[2]])

        # r_batch_softmax = calc.softmax(r_batch_activations, axis=-1) # slow
        argmax = np.argmax(r_batch_activations, axis=-1)  # just take direct max

        out_img = np.concatenate((np.squeeze(img_util.to_rgb(r_batch_img)),
                                  np.squeeze(img_util.to_rgb(r_batch_label)),
                                  np.squeeze(img_util.to_rgb(r_batch_activations[..., 0, np.newaxis], normalize=False)),
                                  np.squeeze(img_util.to_rgb(r_batch_activations[..., 1, np.newaxis], normalize=False)),
                                  np.squeeze(img_util.to_rgb(argmax[..., np.newaxis])),
                                  np.squeeze(img_util.to_rgb(r_batch_prediction[..., np.newaxis]))
                                  ), axis=1)

        img_util.save_image(out_img, "%s/img_%s.png" % (args.predict_dir, b))

        # ###########################################################################
        # CLOSE NET
        # ###########################################################################


# test with sampling for uncertainty (only makes sense when resample_n != None and keep_prob != 1.0
def test_sampling(sess, net_test):
    logging.info('#-----------------------------------------------#')
    logging.info('#        Starting Testing with sampling         #')
    logging.info('#-----------------------------------------------#')

    # # init variables
    # sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))

    trainer = SimpleTrainer(session=sess, train_dir=args.train_dir)
    # load model (if None provided, gets latest from train_dir/checkpoints, if none found doesn't load)
    trainer.load_checkpoint(args.checkpoint)

    # ###########################################################################
    # RUN UNET
    # ###########################################################################
    logging.debug("predicting, sampling %s x %s times, batch_size %s" %
                  (str(opts_test.n_samples), str(opts_test.resample_n), str(opts_test.batch_size)))

    sample_dir = args.predict_dir + os.sep + 'samples'
    os.mkdir(sample_dir)
    for b in range(opts_test.n_samples):

        for s in range(opts_test.resample_n):
            if s == 0:
                batch_img, batch_label, batch_pred = sess.run(
                    [net_test.batch_img, net_test.batch_label, net_test.prediction])
                logging.debug('batch_img: %s %s' % (str(batch_img.shape), str(batch_img.dtype)))
                logging.debug('batch_label: %s %s' % (str(batch_label.shape), str(batch_label.dtype)))
                logging.debug('prediction: %s %s' % (str(batch_pred.shape), str(batch_pred.dtype)))
            else:
                _, _, batch_pred = sess.run(
                    [net_test.batch_img, net_test.batch_label, net_test.prediction])
                logging.debug('prediction: %s %s' % (
                str(prediction_samples[s, ...].shape), str(prediction_samples[s, ...].dtype)))

            r_batch_pred = np.reshape(batch_pred, [-1, batch_pred.shape[2]])
            if s == 0: prediction_samples = np.zeros([opts_test.resample_n] + list(r_batch_pred.shape), dtype=np.uint8)
            prediction_samples[s, ...] = r_batch_pred

            out_sample = img_util.to_rgb(prediction_samples[s, ...])
            img_util.save_image(out_sample, "%s/sample_%s_%s.png" % (sample_dir, b, s))

        logging.info('finished resampling (%s), calculating entropy' % (str(opts_test.resample_n)))

        entropy = calc.entropy_bin_array(prediction_samples)
        mean = np.mean(prediction_samples, axis=0)
        std = np.std(prediction_samples, axis=0)

        r_batch_img = np.reshape(batch_img, [-1, batch_img.shape[2], batch_img.shape[3]])
        r_batch_label = np.reshape(batch_label, [-1, batch_label.shape[2], batch_label.shape[3]])

        out_img = np.concatenate((np.squeeze(img_util.to_rgb(r_batch_img)),
                                  np.squeeze(img_util.to_rgb(r_batch_label)),
                                  np.squeeze(
                                      img_util.to_rgb(mean)),
                                  np.squeeze(
                                      img_util.to_rgb_heatmap(entropy, rgb_256=True)),
                                  np.squeeze(
                                      img_util.to_rgb_heatmap(std, rgb_256=True))
                                  ), axis=1)

        img_util.save_image(out_img, "%s/img_%s.png" % (args.predict_dir, b))

        # ###########################################################################
        # CLOSE NET
        # ###########################################################################


def TEST(): pass


if opts.test:
    logging.info('####################################################################')
    logging.info('#                            TESTING                               #')
    logging.info('####################################################################')

    # create network graph with data layer
    net = model.UNet(dataset_pth=args.dataset,
                     shape_img=opts_test.shape_img, shape_label=opts_test.shape_label, shape_weights=opts_test.shape_weights,
                     batch_size=opts_test.batch_size, shuffle=False, augment=False,
                     resize=opts_test.resize, resize_method=opts_test.resize_method,
                     data_layer_type=opts_test.data_layer_type,
                     n_contracting_blocks=opts_test.n_contracting_blocks, n_start_features=opts_test.n_start_features,
                     norm_fn=opts.norm_fn, normalizer_params=opts.norm_fn_params,
                     resample_n=opts_test.resample_n,
                     is_training=False, keep_prob=opts_test.keep_prob,
                     prefetch_n=None, prefetch_threads=None,
                     debug=opts.debug, copy_script_dir=None
                     )

    with tf_debug.LocalCLIDebugWrapperSession(tf.Session(config=opts.tf_config)) if opts.tfdbg \
            else tf.Session(config=opts.tf_config) as sess:
        # for hdf5 and feed_dict layers no coordinators need to be initialized
        # different when using tfrecords
        if opts.debug:
            test_debug(sess, net)
        else:
            if opts_test.resample_n is not None:
                test_sampling(sess, net)
            else:
                test_core(sess, net)


    logging.info('#-X---------------------------------------------#')
    logging.info('#                Finish Testing                 #')
    logging.info('#-----------------------------------------------#')


# ######################################################################################################################
# ######################################################################################################################
# DEBUG
# --------
def __________________________DEBUG_____________________________(): pass  # dummy function for PyCharm IDE


opts.batch_size = 5


# core code for training
def DEBUG_core(sess):
    logging.info('#-----------------------------------------------#')
    logging.info('#                Start Debugging                #')
    logging.info('#-----------------------------------------------#')

    batch_img, batch_label, batch_weights = data_layers.data_HDF5(args.dataset,
                                                                  opts.shape_img, opts.shape_label, opts.shape_weights,
                                                                  shuffle=False, batch_size=opts.batch_size,
                                                                  prefetch_threads=12, prefetch_n=40,
                                                                  resample_n=40,
                                                                  augment=True)

    for bb in range(opts.n_samples):
        r_batch_img = []
        for b in range(8):
            start = timer()
            e_batch_img, e_batch_label, e_batch_weights = sess.run([batch_img, batch_label, batch_weights])
            end = timer()

            print("got batch in %.4f s : img %s %s" % ((end - start), str(e_batch_img.shape), str(e_batch_img.dtype)))

            r_batch_img.append(np.reshape(e_batch_img, [-1, e_batch_img.shape[2], e_batch_img.shape[3]]))

        print("stitching and creating file")
        out_img = np.concatenate([img_util.to_rgb(batch) for batch in r_batch_img], axis=1)
        img_util.save_image(out_img, "%s/img_aug_%s.jpg" % (args.predict_dir, str(bb)))

    batch_img, batch_label, batch_weights = data_layers.data_HDF5(args.dataset,
                                                                  opts.shape_img, opts.shape_label, opts.shape_weights,
                                                                  shuffle=True, batch_size=opts.batch_size,
                                                                  prefetch_threads=12, prefetch_n=10,
                                                                  resample_n=None,
                                                                  augment=True)

    for b in range(50):
        start = timer()
        e_batch_img, e_batch_label, e_batch_weights = sess.run([batch_img, batch_label, batch_weights])
        end = timer()

        print("got batch in %.4f s : img %s %s, label %s %s, weights %s %s" % ((end - start),
                                                                               str(e_batch_img.shape),
                                                                               str(e_batch_img.dtype),
                                                                               str(e_batch_label.shape),
                                                                               str(e_batch_label.dtype),
                                                                               str(e_batch_weights.shape),
                                                                               str(e_batch_weights.dtype)))

        r_batch_img = np.reshape(e_batch_img, [-1, e_batch_img.shape[2], e_batch_img.shape[3]])
        r_batch_label = np.reshape(e_batch_label, [-1, e_batch_label.shape[2], e_batch_label.shape[3]])
        r_batch_weights = np.reshape(e_batch_weights, [-1, e_batch_weights.shape[2], e_batch_weights.shape[3]])

        out_img = np.concatenate((np.squeeze(img_util.to_rgb(r_batch_img)),
                                  np.squeeze(img_util.to_rgb(r_batch_label)),
                                  np.squeeze(img_util.to_rgb(r_batch_weights, normalize=True))), axis=1)

        img_util.save_image(out_img, "%s/img_%s.png" % (args.predict_dir, str(b)))


# TODO: Remove debugging code
if opts.debug and False:  # temporarily disabled
    logging.info('####################################################################')
    logging.info('#                           DEBUGGING                              #')
    logging.info('####################################################################')

    with tf.Session(config=opts.tf_config) as sess:
        DEBUG_core(sess)

    logging.info('#-X---------------------------------------------#')
    logging.info('#               Finish Debugging                #')
    logging.info('#-----------------------------------------------#')

logging.info('\n\n ... done :)')
